## Import the modules

import scipy.io 
import numpy as np 

import matplotlib.pyplot as plt

import sklearn
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score

from sklearn.metrics import plot_confusion_matrix
from sklearn.metrics import confusion_matrix

##------------------Chargement des données en mémoire------------------------##
data1 = scipy.io.loadmat("Entrees_signal_apprentissage.mat")

data2 = scipy.io.loadmat("Entrees_signal_validation.mat")

#data3 = scipy.io.loadmat("Sorties_apprentissage_EPEM_py.mat")
data3 = scipy.io.loadmat("Sorties_apprentissage_EPEM.mat") 

#data4 = scipy.io.loadmat("Sorties_validation_EPEM_py.mat")
data4 = scipy.io.loadmat("Sorties_validation_EPEM.mat")

#Pour avoir les clés d'un fichier .mat :
print("Les clés d'un fichier .mat : ", data1.keys())

#Pour voir le contenu d'un fichier .mat :
print("Contenu d'un fichier .mat : ", data1['Entrees_signal_apprentissage'])

#Pour avoir le type d'un fichier .mat :
print("Type d'un fichier .mat : ", type(data1['Entrees_signal_apprentissage']))

#Pour avoir la taille du tableau d'un fichier .mat :
print("Taille d'un fichier .mat :", data1['Entrees_signal_apprentissage'].shape)   

##--------------Echantillonage et conversion signal 1D en image 2D------------##
from math import *

def echantillonnage(signal):
  M = len(signal)
  N = floor(sqrt(M))
  #N = floor(M**(1/2))
  signal_new = signal[1:N**2+1]
  return(signal_new)

def conversion(signal):
  N = int(sqrt(len(signal)))
  liste_de_N =[i for i in range(1,N+1)]
  tableau = np.array([liste_de_N]*N)
  F=np.array(tableau)
  for j in range(1,N+1) and range(len(F)):
    for k in range(1,N+1) and range(len(F)):    
      F[j,k] = ((signal[(j-1)*N+k] - min(signal))/(max(signal)-min(signal)))*255
  return(F)

##-------------Préparation des données pour la phase d'apprentissage---------###

donnee1 = data1['Entrees_signal_apprentissage']

ligne_entree_apprentissage_echantillonnee = []
for k in range(len(donnee1)):
  ligne_entree_apprentissage_echantillonnee.append(echantillonnage(donnee1[k]))

ligne_entree_apprentissage_convertie_en_IMAGE_2D = []
for k in range(len(ligne_entree_apprentissage_echantillonnee)):
  ligne_entree_apprentissage_convertie_en_IMAGE_2D.append(conversion(ligne_entree_apprentissage_echantillonnee[k]))

ligne_entree_apprentissage_convertie_en_IMAGE_2D = np.array(ligne_entree_apprentissage_convertie_en_IMAGE_2D)

X_train = ligne_entree_apprentissage_convertie_en_IMAGE_2D

y_train = data3['Sorties_apprentissage_EPEM']
for k in range(len(y_train)):
    if y_train[k][0]==['Fonctionnement_nominal']:
        y_train[k][0]=0
    if y_train[k][0]==['Defaut pompe']:
        y_train[k][0]=1

y_train=y_train.reshape(384,)

y_train=y_train.astype('int')

## --------------------Préparation des données pour la phase de validation----##
donnee2 = data2['Entrees_signal_validation']

ligne_entree_validation_echantillonnee = []
for k in range(len(donnee2)):
  ligne_entree_validation_echantillonnee.append(echantillonnage(donnee2[k]))

ligne_entree_validation_convertie_en_IMAGE_2D = []
for k in range(len(ligne_entree_validation_echantillonnee)):
  ligne_entree_validation_convertie_en_IMAGE_2D.append(conversion(ligne_entree_validation_echantillonnee[k]))

ligne_entree_validation_convertie_en_IMAGE_2D = np.array(ligne_entree_validation_convertie_en_IMAGE_2D)

X_test = ligne_entree_validation_convertie_en_IMAGE_2D

y_test = data4['Sorties_validation_EPEM']
for k in range(len(y_test)):
    if y_test[k][0]==['Fonctionnement_nominal']:
        y_test[k][0]=0
    if y_test[k][0]==['Defaut pompe']:
        y_test[k][0]=1

y_test=y_test.reshape(96,)

y_test=y_test.astype('int')

## Import the modules

import numpy as np
#from keras.models import Sequential
from keras.layers import Dense
from keras.layers import Dropout
from keras.layers import Flatten
from keras.layers.convolutional import Conv2D
from keras.layers.convolutional import MaxPooling2D
from keras import backend as K
#K.set_image_dim_ordering('th')
K.set_image_data_format('channels_first')

##############################################################################
import numpy as np
from keras.datasets import mnist
from keras.utils import np_utils

from keras import backend as K
##K.set_image_dim_ordering('th')
K.set_image_data_format('channels_first')

# fix random seed for reproducibility
seed = 7
np.random.seed(seed)

#Prepare data :
# load data
X_train = X_train.reshape(X_train.shape[0], 1, 31, 31).astype('float32')
X_test = X_test.reshape(X_test.shape[0], 1, 31, 31).astype('float32')   
    
# normalize inputs from 0-255 to 0-1
X_train = X_train / 255
X_test = X_test / 255
# one hot encode outputs
y_train = np_utils.to_categorical(y_train)

y_test = np_utils.to_categorical(y_test)

num_classes = y_test.shape[1]

# Evaluate a model using data and expected predictions
def print_model_error_rate(model, X_test, y_test):
    # Final evaluation of the model
    scores = model.evaluate(X_test, y_test, verbose=0)
    print("Model score : %.2f%%" % (scores[1]*100))
    print("Model error rate : %.2f%%" % (100-scores[1]*100))
 
from keras.models import model_from_json

from PIL import Image

import matplotlib.pyplot as plt

import numpy as np

# This function saves a model on the drive using two files : a json and an h5
def save_keras_model(model, filename):
    # serialize model to JSON
    model_json = model.to_json()
    with open(filename+".json", "w") as json_file:
        json_file.write(model_json)
    # serialize weights to HDF5
    model.save_weights(filename+".h5")
    
# This function loads a model from two files : a json and a h5
# BE CAREFUL : the model NEEDS TO BE COMPILED before any use !
def load_keras_model(filename):
    # load json and create model
    json_file = open(filename+".json", 'r')
    loaded_model_json = json_file.read()
    json_file.close()
    loaded_model = model_from_json(loaded_model_json)
    # load weights into new model
    loaded_model.load_weights(filename+".h5")
    return loaded_model

# Evaluate a model using data and expected predictions
def print_model_error_rate(model, X_test, y_test):
    # Final evaluation of the model
    scores = model.evaluate(X_test, y_test, verbose=0)
    print("Model score : %.2f%%" % (scores[1]*100))
    print("Model error rate : %.2f%%" % (100-scores[1]*100))
    
# Save a data image to a real image on your desktop
def export_image_from_dataset(data, filename):
    im = Image.fromarray(data)
    im.save(filename)

# Instead of saving the data image, it is displayed in console thanks to matplotlib 
def plot_image_from_dataset(data):
    plt.imshow(data, cmap=plt.get_cmap('gray'))
   
# Load an image and converts it to array, for usage in models
def import_custom_image_to_dataset(filename):
    #Load image in grayscale
    img = Image.open(filename).convert('L')
    #Resize like other images in dataset
    img = img.resize((31,31), Image.ANTIALIAS)
    #Convert to array
    x =  np.array(img)
    #Reshape
    x = x.reshape(1,31,31,1).astype('float32')
    #Normalize
    x = x / 255
    
    plt.imshow(x, cmap=plt.get_cmap('gray'))
    return x
# The Functional API ## ........https://keras.io/guides/functional_api/

import numpy as np
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from keras.utils.vis_utils import plot_model


# fix random seed for reproducibility
seed = 7
np.random.seed(seed)

# define the larger model
def the_functional_model():
  
    # create model
    #inputs = keras.Input(shape=(32, 32, 3), name="img")
    inputs = keras.Input(shape=(1, 31, 31), name="img")
    x = layers.Conv2D(30, 5, activation="relu")(inputs)
    block_1_output = layers.MaxPooling2D(2)(x)

    x = layers.Conv2D(15, 3, activation="relu")(block_1_output)
    block_2_output = layers.MaxPooling2D(2)(x)


    x = layers.Dropout(0.2)(block_2_output)
    x = layers.Flatten()(x)
    x = layers.Dense(128, activation="relu")(x)
    x = layers.Dense(50, activation="relu")(x)

    outputs = layers.Dense(num_classes, activation='softmax')(x)     # num_classes

    model = keras.Model(inputs, outputs, name="Large_cnn")
    #model.summary()

    # Compile model
    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])                # Fonction de perte : categorical_crossentropy; optimiseur : Adam; métrique : accuracy
    #model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])                      # Fonction de perte : binary_crossentropy; optimiseur : Adam; métrique : accuracy
    return model


# build the model
model = the_functional_model()

# Compile model
#model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])                # Fonction de perte : categorical_crossentropy; optimiseur : Adam; métrique : accuracy
#model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])                      # Fonction de perte : binary_crossentropy; optimiseur : Adam; métrique : accuracy

# check its summary
model.summary()

# Plot the model
plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)

# Fit the model
#history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=40, batch_size=200)
#history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=100, batch_size=200)          #### 
#history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=50, batch_size=200)

history = model.fit(X_train, y_train, validation_split = 0.2, validation_data=(X_test, y_test), epochs=200, batch_size=32) ### validation_split = 0.2; epochs = 200; batch_size = 32 (valeur par défaut);

# Evaluate the model
#cu.print_model_error_rate(model, X_test, y_test)
print_model_error_rate(model, X_test, y_test)
# Save the model
#cu.save_keras_model(model, "save_model/large_model_cnn")
save_keras_model(model, "save_model_large_model_cnn")

# check the summary of the model
model.summary()

# Plot the model

from keras.utils.vis_utils import plot_model
plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)

print(history.history.keys())

import matplotlib.pyplot as plt

fig, ax = plt.subplots(1, 2, figsize=(20, 3))
ax = ax.ravel()

for i, metric in enumerate(['loss', 'accuracy']):
  ax[i].plot(history.history[metric])
  ax[i].plot(history.history["val_" + metric])
  ax[i].set_title("Model {}".format(metric))
  ax[i].set_xlabel("epochs")
  ax[i].set_ylabel(metric)
  ax[i].legend(["train", "val"])
  
# Make predictions with the model

labels = ['Fonctionnement_nominal','Defaut pompe' ]

y_test = data4['Sorties_validation_EPEM']
for k in range(len(y_test)):
    if y_test[k][0]==['Fonctionnement_nominal']:
        y_test[k][0]=0
    if y_test[k][0]==['Defaut pompe']:
        y_test[k][0]=1

y_test=y_test.reshape(96,)

y_test=y_test.astype('int')

y_pred=model.predict(X_test)

print(y_pred.argmax(axis=1))
print(y_test)

##..https://stackoverflow.com/questions/48987959/classification-metrics-cant-handle-a-mix-of-continuous-multioutput-and-multi-la
#Making confusion matrix that checks accuracy of the model
from sklearn.metrics import confusion_matrix
from sklearn.metrics import ConfusionMatrixDisplay

confusion_matrix= confusion_matrix(y_test, y_pred.argmax(axis=1))
# Display a confusion matrix.
labels = ['Fonctionnement_nominal','Defaut pompe']

disp = ConfusionMatrixDisplay(confusion_matrix=confusion_matrix, display_labels=labels)
disp.plot(include_values=True, cmap="viridis", ax=None, xticks_rotation="vertical")
plt.show()

